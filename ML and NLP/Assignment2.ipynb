{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f195ea81-9701-462c-9f22-8643236668ff",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afcb9919-01cc-4a27-a2fe-9cafc3548581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\TUF\n",
      "[nltk_data]     GAMING\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47643c36-cb26-4ccb-89c2-60d19d232cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function using RegexpTokenizer to avoid punkt error\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [tok for tok in tokens if tok not in stop_words and tok.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff6c72de-4477-4350-96f8-c79d5fdf12c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec model (this may take a while)...\n",
      "Word2Vec model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load Google News Word2Vec via gensim-data\n",
    "print(\"Loading Word2Vec model (this may take a while)...\")\n",
    "w2v_model = api.load(\"word2vec-google-news-300\")\n",
    "vector_size = w2v_model.vector_size\n",
    "print(\"Word2Vec model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31f65cd0-9b43-408c-9b77-4d00ceb52acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokens to mean vector\n",
    "def vectorize_sentence(tokens, model, vector_size=300):\n",
    "    valid = [model[t] for t in tokens if t in model]\n",
    "    if not valid:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(valid, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "848fb028-e5f0-46ea-9c9f-4157e4151366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Loaded 5572 messages.\n"
     ]
    }
   ],
   "source": [
    "# Load SMS Spam dataset from spam.csv\n",
    "# Ensure spam.csv is in the same directory and has columns 'v1' and 'v2'\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv('spam.csv', encoding='latin-1')[['v1', 'v2']]\n",
    "df.columns = ['Label', 'Message']\n",
    "print(f\"Loaded {len(df)} messages.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fd82187-d9fa-4577-97ff-de79dd2e2de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing & vectorizing...\n"
     ]
    }
   ],
   "source": [
    "# Vectorize all messages\n",
    "print(\"Preprocessing & vectorizing...\")\n",
    "X = np.vstack([vectorize_sentence(preprocess(m), w2v_model, vector_size) for m in df['Message']])\n",
    "y = df['Label'].map({'ham': 0, 'spam': 1}).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86c4482e-0d65-48fc-b9cb-45a34d21bbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9444\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "print(f\"Test Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb7da12d-ff6b-499e-8a90-716a9116edaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_message_class(classifier, w2v_model, message):\n",
    "    tokens = preprocess(message)\n",
    "    vec = vectorize_sentence(tokens, w2v_model, vector_size)\n",
    "    pred = classifier.predict(vec.reshape(1,-1))[0]\n",
    "    return 'spam' if pred==1 else 'ham'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "665be327-2f45-4b31-afc9-debb20955674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations! You've won a free ticket. Reply now! -> spam\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "if __name__ == '__main__':\n",
    "    msg = \"Congratulations! You've won a free ticket. Reply now!\"\n",
    "    print(msg, \"->\", predict_message_class(clf, w2v_model, msg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50289a2d-018a-419a-9c0a-9b1cfc2f69d3",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7668d214-881d-4547-b234-d1be0486bd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec model (this may take a while)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\TUF\n",
      "[nltk_data]     GAMING\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\TUF\n",
      "[nltk_data]     GAMING\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\TUF\n",
      "[nltk_data]     GAMING\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model loaded.\n",
      "Loading dataset...\n",
      "Loaded 14640 tweets.\n",
      "Preprocessing & vectorizing tweets...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import contractions\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Preprocessing tools\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_tweet(text):\n",
    "    text = text.lower()\n",
    "    text = contractions.fix(text)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # remove URLs\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)  # remove mentions and hashtags\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # remove punctuation and special chars\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(tok) for tok in tokens if tok not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Load Word2Vec model\n",
    "print(\"Loading Word2Vec model (this may take a while)...\")\n",
    "w2v_model = api.load(\"word2vec-google-news-300\")\n",
    "vector_size = w2v_model.vector_size\n",
    "print(\"Word2Vec model loaded.\")\n",
    "\n",
    "def vectorize_sentence(tokens, model, vector_size=300):\n",
    "    valid = [model[t] for t in tokens if t in model]\n",
    "    if not valid:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(valid, axis=0)\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(\"Tweets.csv\")  # ensure the file is in the same directory\n",
    "print(f\"Loaded {len(df)} tweets.\")\n",
    "\n",
    "# Focus only on necessary columns\n",
    "df = df[['airline_sentiment', 'text']].dropna()\n",
    "\n",
    "# Preprocess and vectorize\n",
    "print(\"Preprocessing & vectorizing tweets...\")\n",
    "X = np.vstack([vectorize_sentence(preprocess_tweet(t), w2v_model, vector_size) for t in df['text']])\n",
    "y = df['airline_sentiment'].map({\"negative\": 0, \"neutral\": 1, \"positive\": 2}).values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train model\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "def predict_tweet_sentiment(model, w2v_model, tweet):\n",
    "    tokens = preprocess_tweet(tweet)\n",
    "    vec = vectorize_sentence(tokens, w2v_model, vector_size)\n",
    "    pred = model.predict(vec.reshape(1, -1))[0]\n",
    "    return {0: \"negative\", 1: \"neutral\", 2: \"positive\"}[pred]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "    example_tweet = \"@AmericanAir I love how you handle customer service, great job!\"\n",
    "    print(example_tweet, \"->\", predict_tweet_sentiment(clf, w2v_model, example_tweet))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
